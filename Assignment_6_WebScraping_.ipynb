{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lt33tx/Landon_Tinch_DTSC3020_Fall2025-/blob/main/Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "\n",
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389059eb-f3e9-4ec0-e0d7-206f1db3982b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "1 #Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "##Common Imports & Polite Headers\n",
        "\n",
        "\n"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840e56e3-5ca0-45bf-da4f-46318ef8f8a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06080688-704d-4f2c-c9ab-6adf098f9ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data done! Full data saved to data_q1.csv\n",
            "                                                        Country Alpha-2 Alpha-3  Numeric\n",
            "247                                                      Zambia      ZM     ZMB      894\n",
            "246                                                       Yemen      YE     YEM      887\n",
            "192                                                       Samoa      WS     WSM      882\n",
            "244                                           Wallis and Futuna      WF     WLF      876\n",
            "240                          Venezuela (Bolivarian Republic of)      VE     VEN      862\n",
            "238                                                  Uzbekistan      UZ     UZB      860\n",
            "237                                                     Uruguay      UY     URY      858\n",
            "35                                                 Burkina Faso      BF     BFA      854\n",
            "243                                       Virgin Islands (U.S.)      VI     VIR      850\n",
            "236                              United States of America (the)      US     USA      840\n",
            "219                                Tanzania, United Republic of      TZ     TZA      834\n",
            "108                                                 Isle of Man      IM     IMN      833\n",
            "113                                                      Jersey      JE     JEY      832\n",
            "92                                                     Guernsey      GG     GGY      831\n",
            "234  United Kingdom of Great Britain and Northern Ireland (the)      GB     GBR      826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-93489842.py:8: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html, header=0)  # read all HTML tables\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your answer here\n",
        "\n",
        "URL_Q1 = \"https://www.iban.com/country-codes\"  # link to pull country code table from\n",
        "\n",
        "def q1_read_table(html: str) -> pd.DataFrame:  # function to read table from HTML\n",
        "    \"\"\"Snags the main table, tries to use the first row as headers.\"\"\"\n",
        "\n",
        "    tables = pd.read_html(html, header=0)  # read all HTML tables\n",
        "\n",
        "    df = next(df for df in tables if df.shape[1] >= 3)  # grab first table with enough columns\n",
        "\n",
        "    df = flatten_headers(df)  # flatten messy multi-row headers\n",
        "\n",
        "    df.columns = ['Country', 'Alpha-2', 'Alpha-3', 'Numeric']  # rename columns cleanly\n",
        "\n",
        "    return df  # return cleaned raw table\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:  # function to clean table\n",
        "    \"\"\"Cleaning time! Stripping spaces, upper-casing, and making 'Numeric' an actual number.\"\"\"\n",
        "\n",
        "    for col in df.select_dtypes(include='object').columns:  # loop over string columns\n",
        "         df[col] = df[col].str.strip()  # remove extra spaces\n",
        "\n",
        "    for col in ['Alpha-2', 'Alpha-3']:  # enforce uppercase for codes\n",
        "        df[col] = df[col].str.upper()  # convert to uppercase\n",
        "\n",
        "    df['Numeric'] = df['Numeric'].astype(str).str.replace(r'[^\\d]', '', regex=True)  # strip non-digits\n",
        "    df['Numeric'] = pd.to_numeric(df['Numeric'], errors='coerce').astype('Int64')  # convert to numeric\n",
        "\n",
        "    df.dropna(subset=['Numeric'], inplace=True)  # remove rows missing Numeric code\n",
        "\n",
        "    return df  # return cleaned dataframe\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:  # function to sort by numeric code\n",
        "    \"\"\"Sort descending by Numeric and return the top N rows, ez.\"\"\"\n",
        "\n",
        "    df_sorted = df.sort_values(by='Numeric', ascending=False)  # sort biggest numeric codes first\n",
        "\n",
        "    return df_sorted.head(top)  # return top N\n",
        "\n",
        "try:\n",
        "    html_q1 = fetch_html(URL_Q1)  # fetch HTML text from website\n",
        "except requests.exceptions.HTTPError as e:  # catch fetch errors\n",
        "    print(f\"Seriously, a 404? Error fetching URL: {e}\")  # print error message\n",
        "    sys.exit(1)  # exit program\n",
        "\n",
        "df_q1 = q1_read_table(html_q1)  # read raw table from HTML\n",
        "\n",
        "df_q1_clean = q1_clean(df_q1.copy())  # clean a copy of the table\n",
        "\n",
        "output_filename = 'data_q1.csv'  # choose filename for saving\n",
        "\n",
        "df_q1_clean.to_csv(output_filename, index=False)  # save cleaned data to CSV\n",
        "print(f\"Data done! Full data saved to {output_filename}\")  # confirm save\n",
        "\n",
        "df_q1_top15 = q1_sort_top(df_q1_clean)  # get top 15 countries by numeric code\n",
        "print(df_q1_top15.to_string())  # print formatted output"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q2_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917252b5-e5be-4bc4-fac5-6115d7a60889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q2 data saved to data_q2.csv\n",
            "    rank                                                                        title  points  comments\n",
            "13    14  YouTube Removes Windows 11 Bypass Tutorials, Claims 'Risk of Physical Harm'     411       156\n",
            "9     10                                                      Why I love OCaml (2023)     297       203\n",
            "22    23               VLC's Jean-Baptiste Kempf Receives the European SFS Award 2025     249        41\n",
            "24    25                                                        James Watson has died     241       134\n",
            "5      6     Myna: Monospace typeface designed for symbol-heavy programming languages     206        81\n",
            "7      8                                                       Ruby Solved My Problem     173        68\n",
            "0      1                                                          Why is Zig so cool?     161        56\n",
            "6      7                                                          How did I get here?     127        33\n",
            "2      3                                                 Becoming a Compiler Engineer     121        50\n",
            "23    24                                               Angel Investors, a Field Guide     106        22\n",
            "17    18                                                      Venn Diagram for 7 Sets      96        20\n",
            "19    20                FAA to restrict commercial rocket launches to overnight hours      93        35\n",
            "1      2                    Snapchat open-sources Valdi a cross-platform UI framework      78        20\n",
            "15    16                     Transducer: Composition, abstraction, performance (2018)      76         0\n",
            "29    30              I'm making a small RPG and I need feeback regarding performance      68        60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2282838503.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['title'].fillna('No Title', inplace=True)  # fill missing titles\n",
            "/tmp/ipython-input-2282838503.py:56: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['link'].fillna('No Link', inplace=True)  # fill missing links\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your answer here\n",
        "\n",
        "URL_Q2 = \"https://news.ycombinator.com/\"  # URL for Hacker News front page\n",
        "\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:  # function to parse story info from HTML\n",
        "    \"\"\"Parse front page items into DataFrame columns: rank, title, link, points, comments.\"\"\"\n",
        "\n",
        "    soup = BeautifulSoup(html, 'lxml')  # load HTML into BeautifulSoup\n",
        "    items = []  # list to hold parsed story dictionaries\n",
        "\n",
        "    title_rows = soup.select('tr.athing')  # find all rows that represent a story\n",
        "\n",
        "    for row in title_rows:  # loop through each story row\n",
        "        rank_tag = row.select_one('.rank')  # grab the rank element\n",
        "        rank = rank_tag.text.strip().replace('.', '') if rank_tag else ''  # clean the rank\n",
        "\n",
        "        title_tag = row.select_one('.titleline a')  # grab the title link\n",
        "        title = title_tag.text.strip() if title_tag else ''  # extract text\n",
        "        link = title_tag['href'] if title_tag and title_tag.has_attr('href') else ''  # extract URL\n",
        "\n",
        "        subtext_row = row.find_next_sibling('tr')  # get row containing points/comments\n",
        "\n",
        "        points = ''  # default points\n",
        "        comments = ''  # default comment count\n",
        "\n",
        "        if subtext_row:  # ensure subtext row exists\n",
        "            score_tag = subtext_row.select_one('.score')  # points element\n",
        "            if score_tag:\n",
        "                points = score_tag.text.strip().split(' ')[0]  # extract number part\n",
        "\n",
        "            comment_tag = subtext_row.find_all('a')[-1]  # the comments link\n",
        "            comment_text = comment_tag.text.lower()  # lowercase text\n",
        "\n",
        "            if 'comment' in comment_text or 'discuss' in comment_text:  # check if comment-like\n",
        "                if 'discuss' in comment_text or comment_text == 'hide':\n",
        "                    comments = '0'  # treat discuss as zero comments\n",
        "                else:\n",
        "                    comments = comment_text.split(' ')[0]  # grab number\n",
        "\n",
        "        items.append({  # build row dictionary\n",
        "            'rank': rank,\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'points': points,\n",
        "            'comments': comments,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(items)  # return DataFrame of results\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:  # function to clean parsed data\n",
        "    \"\"\"Clean numeric fields and fill missing values.\"\"\"\n",
        "\n",
        "    df['title'].fillna('No Title', inplace=True)  # fill missing titles\n",
        "    df['link'].fillna('No Link', inplace=True)  # fill missing links\n",
        "\n",
        "    for col in ['rank', 'points', 'comments']:  # numeric-like columns\n",
        "        df[col] = df[col].astype(str).str.replace(r'[^\\d]', '', regex=True)  # strip non-digits\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)  # convert to int\n",
        "\n",
        "    return df  # return cleaned DataFrame\n",
        "\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:  # function to sort top stories\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "\n",
        "    df_sorted = df.sort_values(by='points', ascending=False)  # sort by points\n",
        "\n",
        "    return df_sorted.head(top)  # return top N rows\n",
        "\n",
        "\n",
        "try:\n",
        "    html_q2 = fetch_html(URL_Q2)  # fetch the Hacker News HTML\n",
        "except requests.exceptions.HTTPError as e:  # catch fetch errors\n",
        "    print(f\"Server is down or something? Error fetching URL: {e}\")  # error message\n",
        "    sys.exit(1)  # exit if URL fails\n",
        "\n",
        "df_q2 = q2_parse_items(html_q2)  # parse the HTML into a table\n",
        "\n",
        "df_q2_clean = q2_clean(df_q2.copy())  # clean up numeric and text fields\n",
        "\n",
        "output_filename = 'data_q2.csv'  # set output filename\n",
        "\n",
        "df_q2_clean.to_csv(output_filename, index=False)  # save cleaned data to CSV\n",
        "print(f\"Q2 data saved to {output_filename}\")  # confirm save\n",
        "\n",
        "df_q2_top15 = q2_sort_top(df_q2_clean)  # get top 15 stories\n",
        "print(df_q2_top15[['rank', 'title', 'points', 'comments']].to_string())  # print final table"
      ],
      "id": "q2_skeleton_answer"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-Auvsu2bgZX"
      },
      "id": "7-Auvsu2bgZX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}